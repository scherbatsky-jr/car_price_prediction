{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14kWXUnJwIBU"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QSjJhJqwLxS",
        "outputId": "3e02ee77-2a5e-43c7-c9f5-d1ba658f915a"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "np.__version__, pd.__version__, sns.__version__, matplotlib.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQDbjlagweLj"
      },
      "source": [
        "**1. Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6VMUg2ewigV"
      },
      "outputs": [],
      "source": [
        "# Load the data as pandas dataframe\n",
        "df = pd.read_csv('../../dataset/cars.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "zZKRDpZow8pz",
        "outputId": "774f5674-be19-4cde-882f-b582e1e5df20"
      },
      "outputs": [],
      "source": [
        "# Check the first five rows in the data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLnQl1kvw-Dq",
        "outputId": "9133f734-804e-493c-c2ae-a46f2818daa7"
      },
      "outputs": [],
      "source": [
        "# Check the number of rows in the data\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V81nNAdLxGWy",
        "outputId": "047574f2-bbe6-47db-a09e-e56068a4676e"
      },
      "outputs": [],
      "source": [
        "# Show the non null values and data type of each column\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz3kjHR0xMjs",
        "outputId": "6ae5aaeb-e830-41cf-b3e3-e51c5059e736"
      },
      "outputs": [],
      "source": [
        "# List out the column names\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the name of columns are perfect to be used, no updates are applied to make them more readable and accessible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfgjK1YpxXyq"
      },
      "source": [
        "**2 Explatory Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.1 Cleaning the data\n",
        "\n",
        "First all the unnecessary feature data are to be removed such as units and brand related information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here we define a function to return float values for features with pattern \"floatvalue + unit\"\n",
        "# Example for feature km_driven, if data is \"12345 km\", then 12345.00 is returned\n",
        "\n",
        "def getFloatValues(featureValues):\n",
        "    # all values are converted to string in case there are any float or integer values\n",
        "    featureValues = featureValues.astype(str)\n",
        "\n",
        "    # the first part of values are separated and converted to float values and mapped\n",
        "    # in case the values can not be converted to float, then values are set to 0\n",
        "    for index, x in enumerate(featureValues):\n",
        "        try:\n",
        "            featureValues[index] = float(x.split(' ')[0])\n",
        "        except ValueError:\n",
        "            featureValues[index] = 0\n",
        "\n",
        "    return featureValues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For feature name, the brand name of the car is kept. The first word in the name is assumed to be brand name\n",
        "df['name'] = df['name'].map(lambda x : x.split(' ')[0])\n",
        "\n",
        "# For feature mileage, the unit kmpl is removed and values converted into float values\n",
        "df['mileage'] = getFloatValues(df['mileage'])\n",
        "df['mileage'] = df['mileage'].astype('float')\n",
        "\n",
        "# For feature engine, the unit CC is removed and values converted into float values\n",
        "df['engine'] = getFloatValues(df['engine'])\n",
        "df['engine'] = df['engine'].astype('float')\n",
        "\n",
        "# For feature max_power, the unit bhp is removed and values converted into float values\n",
        "df['max_power'] = getFloatValues(df['max_power'])\n",
        "df['max_power'] = df['max_power'].astype('float')\n",
        "\n",
        "# For feature torque, it is dropped due insignifcance to car company\n",
        "df = df.drop('torque', axis = 1)\n",
        "\n",
        "# For feature fuel, all the rows with values LPG and CNG are removed\n",
        "df = df[~df['fuel'].isin(['CNG', 'LPG'])]\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egsP54AzyXl_"
      },
      "source": [
        "2.2 Univariate analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAjDriDUyjRX"
      },
      "source": [
        "Countplot\n",
        "\n",
        "Count plot can be used to see the number of rows for a label for a categorical feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "Dfcvq-tGykib",
        "outputId": "0abe0d8e-49c7-403e-e14e-4e171038c5ac"
      },
      "outputs": [],
      "source": [
        "# Let's see how many individual and dealer sellers are there\n",
        "sns.countplot(data = df, x = 'seller_type')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CScVgtxRzMnj"
      },
      "source": [
        "Distribution plot\n",
        "\n",
        "Distribution plot can plot the distribution of continous values. It helps in identifying the type of distribution for the feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "aBB4GiShzO91",
        "outputId": "58098aae-66e1-4363-fd22-4ea05da46e76"
      },
      "outputs": [],
      "source": [
        "# Distribution plot for selling prices\n",
        "sns.displot(data = df, x = 'selling_price')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y6nLQsXzeUm"
      },
      "source": [
        "2.2 Multivariate Analysis\n",
        "\n",
        "Multiple variable exploratory analysis\n",
        "\n",
        "Boxplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "XNzZs9qhzga-",
        "outputId": "b6019f73-f4af-4747-a48f-99f3e6953250"
      },
      "outputs": [],
      "source": [
        "# Box plot for 'owner' and 'selling_price'\n",
        "sns.boxplot(x = df[\"owner\"], y = df[\"selling_price\"]);\n",
        "plt.ylabel(\"Selling Price\")\n",
        "plt.xlabel(\"Owner\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny8O3WFn0E4E"
      },
      "source": [
        "Scatterplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "8EwMYKN70PFt",
        "outputId": "67e13301-c7b0-40f4-8915-0cad1d903ea1"
      },
      "outputs": [],
      "source": [
        "# Scatter plot for mileage and selling price with respect to fuel type\n",
        "\n",
        "sns.scatterplot(x = df['mileage'], y = df['selling_price'], hue =df['fuel'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkO-6uzG0m-E"
      },
      "source": [
        "Corelation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "Ksw1RBoz0reL",
        "outputId": "99b5101c-f630-422d-f4e5-86c171239a2e"
      },
      "outputs": [],
      "source": [
        "# Let's check out heatmap\n",
        "\n",
        "plt.figure(figsize = (15, 8))\n",
        "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Currently, feature max_power and engine have shown strong correlation to selling price. However, the above graph does not include categorical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFuwhA_z1JRi"
      },
      "source": [
        "Label Encoding\n",
        "\n",
        "Lets encode the labels for the present categorical featues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PUGlAnL1K-A",
        "outputId": "1d963733-3821-4771-cf96-7fc42e0c035f"
      },
      "outputs": [],
      "source": [
        "# Importing the LabelEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All the categorical features except 'owner' are label encoded through LabelEncoder class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the features into list\n",
        "categorical_features = ['name', 'seller_type', 'fuel', 'transmission']\n",
        "\n",
        "# Each feature in the list are label encoded \n",
        "for feature in categorical_features:\n",
        "    df[feature] = le.fit_transform(df[feature])\n",
        "    le.transform(le.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the feature 'owner' has the requirement to assign custom encoding to each label, each label has mapped to the required encoding value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6-XEQ75ZBFP"
      },
      "outputs": [],
      "source": [
        "mapping = {\n",
        "    'First Owner': 1,\n",
        "    'Second Owner': 2,\n",
        "    'Third Owner': 3,\n",
        "    'Fourth & Above Owner': 4,\n",
        "    'Test Drive Car': 5\n",
        "}\n",
        "\n",
        "df['owner'] = df['owner'].map(lambda x : mapping[x])\n",
        "\n",
        "# Removing the rows with Test Drive Car value\n",
        "df = df[~df['owner'].isin([5])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that all the categorical labels have been encoded into integer values, lets look into our current data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the correlation matrix will display the values for these converted features as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15, 8))\n",
        "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The most correlated features are still found to be engine and max_power"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYNgpvmi2p3d"
      },
      "source": [
        "**Predictive Power Socre**\n",
        "\n",
        "Let's check the predictive power scores of features. This graph plots the direct predictive power of a feature against another feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "spyfAlKA2wHd",
        "outputId": "38e1bb34-938c-4a53-d263-3b645cfbba94"
      },
      "outputs": [],
      "source": [
        "import ppscore as pps\n",
        "\n",
        "dfcopy = df.copy()\n",
        "\n",
        "matrix_df = pps.matrix(dfcopy)[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\n",
        "\n",
        "#plot\n",
        "plt.figure(figsize = (15,8))\n",
        "sns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doWfAZEUZ99a"
      },
      "source": [
        "**3. Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ml5slJqaAFF"
      },
      "outputs": [],
      "source": [
        "# According the corelation matrix and pps scores, the most strong features are max_power, engine and mileage\n",
        "# Therefore, X is set to those features\n",
        "\n",
        "X = df[['max_power', 'engine', 'mileage']]\n",
        "\n",
        "# y is the selling price. As selling price values are too big, they will transformed with log\n",
        "y = np.log(df['selling_price'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQPVICVlaobx"
      },
      "source": [
        "**4. Test Train Split**\n",
        "\n",
        "The training and test data are split into 7:3 ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa6oFwowarIy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwCGD6lQbEi1"
      },
      "source": [
        "**5 Preprocessing**\n",
        "\n",
        "5.1 Checking for null values\n",
        "\n",
        "Here all the null values for the features are filled with appropriate values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czxmwaZSbHZF",
        "outputId": "66ec89af-ec75-4ae0-e26d-8bf90cd33d93"
      },
      "outputs": [],
      "source": [
        "# Checking for null values in X for training set\n",
        "X_train[['max_power', 'engine', 'mileage']].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Bn7lcNkbNeS",
        "outputId": "e7e6c9ae-fd86-4a8a-abcd-889cd920d756"
      },
      "outputs": [],
      "source": [
        "# Checking for null values in X for test set\n",
        "X_test[['max_power', 'engine', 'mileage']].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ok3qLI-bXEx",
        "outputId": "7395b72f-738d-4c3b-dad6-786702b95560"
      },
      "outputs": [],
      "source": [
        "#Checking for null values in y\n",
        "y_train.isna().sum()\n",
        "y_test.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "c9Oqo2cEbi3b",
        "outputId": "e9612d7d-6518-460c-a82a-ac47c3cc6339"
      },
      "outputs": [],
      "source": [
        "# Distribution plot for max_power\n",
        "sns.displot(data=df, x = 'max_power')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution plot for mileage\n",
        "sns.displot(data=df, x = 'engine')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution plot for mileage\n",
        "sns.displot(data=df, x = 'mileage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "mDYpdYoQb4lE",
        "outputId": "03d0c72b-2e88-4d32-e14b-6c7566a21de9"
      },
      "outputs": [],
      "source": [
        "# Distribution plot selling price for training\n",
        "sns.displot(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The null values for features max power and engine are filled with median values\n",
        "X_train['max_power'].fillna(X_train['max_power'].median(), inplace=True)\n",
        "X_train['engine'].fillna(X_train['engine'].median(), inplace=True)\n",
        "\n",
        "# The null values for feature mileage are filled with mean values as mileage values resemnle normal distribution\n",
        "X_train['mileage'].fillna(X_train['mileage'].mean(), inplace= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The same process above is repeated for test data as well\n",
        "X_test['max_power'].fillna(X_test['max_power'].median(), inplace=True)\n",
        "X_test['engine'].fillna(X_train['engine'].median(), inplace=True)\n",
        "X_test['mileage'].fillna(X_train['mileage'].mean(), inplace= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final check for null values for X in training and test sets\n",
        "X_train[['max_power', 'engine', 'mileage']].isna().sum()\n",
        "X_test[['max_power', 'engine', 'mileage']].isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the values for y have no null values, the y_train and y_test are not updated in this process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbgXJpeVd8cN"
      },
      "source": [
        "**5.2 Checking Outliers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "2Et5dokLd_EF",
        "outputId": "3e5d9bae-1e64-44f3-fd80-3a962aa55046"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary of columns\n",
        "\n",
        "col_dict = {'max_power': 1, 'engine': 2, 'mileage': 3}\n",
        "\n",
        "# Box plots to detect outliers in each variables\n",
        "\n",
        "for variable, i in col_dict.items():\n",
        "  plt.subplot(5,4,i)\n",
        "  plt.boxplot(X_train[variable])\n",
        "  plt.title(variable)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_mK8Skxedti"
      },
      "outputs": [],
      "source": [
        "# This method takes feature name and training set as parameters and print the number and percentage of outliers present for the feature\n",
        "def outlier_count(col, data = X_train):\n",
        "\n",
        "    # calculate your 25% quatile and 75% quartile\n",
        "    q75, q25 = np.percentile(data[col], [75, 25])\n",
        "\n",
        "    # calculate your inter quartile\n",
        "    iqr = q75 - q25\n",
        "\n",
        "    # Calculating max_value and min_value for the feature\n",
        "    min_val = q25 - (iqr*1.5)\n",
        "    max_val = q75 + (iqr*1.5)\n",
        "\n",
        "    # The number of outliers is counted on basis if the value is greater than max value or less than the min value\n",
        "    outlier_count = len(np.where((data[col] > max_val) | (data[col] < min_val))[0])\n",
        "\n",
        "    # calculate the percentage of the outliers\n",
        "    outlier_percent = round(outlier_count/len(data[col])*100, 2)\n",
        "\n",
        "    if(outlier_count > 0):\n",
        "        print(\"\\n\"+15*'-' + col + 15*'-'+\"\\n\")\n",
        "        print('Number of outliers: {}'.format(outlier_count))\n",
        "        print('Percent of data that is outlier: {}%'.format(outlier_percent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMFkBuh3eoZY",
        "outputId": "8e258268-18a1-4edc-92f2-1dad56546d61"
      },
      "outputs": [],
      "source": [
        "# Method call for finding out outliers in all features in training set\n",
        "for col in X_train.columns:\n",
        "    outlier_count(col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From above, feature engine has the highest number of outliers at 14.88%. While the percentage is high and may affect the training model but it has shown good scores in corelation matrix and pps score graph. Usually, the values of engine do tend to affect the prices in real world as well. Hence, engine will be used to train the model.\n",
        "\n",
        "To mitigate the effects of outliers, all the training set will be scaled by Standard Scaler to covert the data into more consistent form."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvFBJNEPetpB"
      },
      "source": [
        "**5.3 Scaling**\n",
        "\n",
        "For scaling the features of training and test set, Robust Scaler will be used. The process of scaling generally helps in faster convergence while training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsEC7SW1evOy"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFJB3TCXey4Q",
        "outputId": "7f71af7c-abee-4f5e-fb0d-303c24a29933"
      },
      "outputs": [],
      "source": [
        "# Shape check for X_train, X_test, y_train, y_test before model fitting\n",
        "print(\"Shape of X_train: \", X_train.shape)\n",
        "print(\"Shape of X_test: \", X_test.shape)\n",
        "print(\"Shape of y_train: \", y_train.shape)\n",
        "print(\"Shape of y_test: \", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "intercept = np.ones((X_train.shape[0], 1))\n",
        "X_train   = np.concatenate((intercept, X_train), axis=1)\n",
        "intercept = np.ones((X_test.shape[0], 1))\n",
        "X_test    = np.concatenate((intercept, X_test), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**6. Modeling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score\n",
        "import math\n",
        "import mlflow\n",
        "\n",
        "\n",
        "class LinearRegression(object):\n",
        "    \n",
        "    #in this class, we add cross validation as well for some spicy code....\n",
        "    kfold = KFold(n_splits=5)\n",
        "            \n",
        "    def __init__(self, regularization, lr, method, theta_type, momentum, num_epochs=500, batch_size=50, cv=kfold):\n",
        "        self.lr         = lr\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.method     = method\n",
        "        self.theta_type = theta_type\n",
        "        self.momentum = momentum\n",
        "        self.cv         = cv\n",
        "        self.regularization = regularization\n",
        "\n",
        "    def mse(self, ytrue, ypred):\n",
        "        return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
        "    \n",
        "    def r2(self, ytrue, ypred):\n",
        "        return r2_score(ytrue, ypred)\n",
        "    \n",
        "    def getXavierTheta(self, num):\n",
        "        lower, upper = -(1.0 / math.sqrt(num)), (1.0 / math.sqrt(num))\n",
        "\n",
        "        numbers = np.random.rand(num)\n",
        "\n",
        "        scaled = lower + numbers * (upper - lower)\n",
        "\n",
        "        return scaled\n",
        "    \n",
        "    def fit(self, X_train, y_train):\n",
        "            \n",
        "        #create a list of kfold scores\n",
        "        self.kfold_scores = list()\n",
        "        \n",
        "        #reset val loss\n",
        "        self.val_loss_old = np.infty\n",
        "\n",
        "        #kfold.split in the sklearn.....\n",
        "        #5 splits\n",
        "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X_train)):\n",
        "            \n",
        "            X_cross_train = X_train[train_idx]\n",
        "            y_cross_train = y_train[train_idx]\n",
        "            X_cross_val   = X_train[val_idx]\n",
        "            y_cross_val   = y_train[val_idx]\n",
        "\n",
        "            self.theta = self.getXavierTheta(X_cross_train.shape[1]) if self.theta_type == 'xavier' else np.zeros(X_cross_train.shape[1])\n",
        "            \n",
        "            # self.theta = np.zeros(X_cross_train.shape[1])\n",
        "            \n",
        "            #define X_cross_train as only a subset of the data\n",
        "            #how big is this subset?  => mini-batch size ==> 50\n",
        "            \n",
        "            #one epoch will exhaust the WHOLE training set\n",
        "            with mlflow.start_run(run_name=f\"Fold-{fold}\", nested=True):\n",
        "                \n",
        "                params = {\"method\": self.method, \"lr\": self.lr, \"reg\": type(self).__name__}\n",
        "                mlflow.log_params(params=params)\n",
        "                \n",
        "                for epoch in range(self.num_epochs):\n",
        "                \n",
        "                    #with replacement or no replacement\n",
        "                    #with replacement means just randomize\n",
        "                    #with no replacement means 0:50, 51:100, 101:150, ......300:323\n",
        "                    #shuffle your index\n",
        "                    perm = np.random.permutation(X_cross_train.shape[0])\n",
        "                            \n",
        "                    X_cross_train = X_cross_train[perm]\n",
        "                    y_cross_train = y_cross_train[perm]\n",
        "                    \n",
        "                    if self.method == 'stochastic':\n",
        "                        for batch_idx in range(X_cross_train.shape[0]):\n",
        "                            X_method_train = X_cross_train[batch_idx].reshape(1, -1) #(11,) ==> (1, 11) ==> (m, n)\n",
        "                            y_method_train = y_cross_train[batch_idx] \n",
        "                            train_loss = self._train(X_method_train, y_method_train)\n",
        "                    elif self.method == 'mini-batch':\n",
        "                        for batch_idx in range(0, X_cross_train.shape[0], self.batch_size):\n",
        "                            #batch_idx = 0, 50, 100, 150\n",
        "                            X_method_train = X_cross_train[batch_idx:batch_idx+self.batch_size, :]\n",
        "                            y_method_train = y_cross_train[batch_idx:batch_idx+self.batch_size]\n",
        "                            train_loss = self._train(X_method_train, y_method_train)\n",
        "                    else:\n",
        "                        X_method_train = X_cross_train\n",
        "                        y_method_train = y_cross_train\n",
        "                        train_loss = self._train(X_method_train, y_method_train)\n",
        "\n",
        "                    mlflow.log_metric(key=\"train_loss\", value=train_loss, step=epoch)\n",
        "\n",
        "                    yhat_val = self.predict(X_cross_val)\n",
        "                    val_loss_new = self.mse(y_cross_val, yhat_val)\n",
        "                    mlflow.log_metric(key=\"val_loss\", value=val_loss_new, step=epoch)\n",
        "                    \n",
        "                    #record dataset\n",
        "                    mlflow_train_data = mlflow.data.from_numpy(features=X_method_train, targets=y_method_train)\n",
        "                    mlflow.log_input(mlflow_train_data, context=\"training\")\n",
        "                    \n",
        "                    mlflow_val_data = mlflow.data.from_numpy(features=X_cross_val, targets=y_cross_val)\n",
        "                    mlflow.log_input(mlflow_val_data, context=\"validation\")\n",
        "                    \n",
        "                    #early stopping\n",
        "                    if np.allclose(val_loss_new, self.val_loss_old):\n",
        "                        break\n",
        "                    self.val_loss_old = val_loss_new\n",
        "            \n",
        "                self.kfold_scores.append(val_loss_new)\n",
        "                print(f\"Fold {fold}: {val_loss_new}\")\n",
        "            \n",
        "                    \n",
        "    def _train(self, X, y):\n",
        "        yhat = self.predict(X)\n",
        "        m    = X.shape[0]        \n",
        "        grad = (1/m) * X.T @(yhat - y) + self.regularization.derivation(self.theta)\n",
        "        self.theta = self.theta - self.lr * grad\n",
        "        return self.mse(y, yhat)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        return X @ self.theta  #===>(m, n) @ (n, )\n",
        "    \n",
        "    def _coef(self):\n",
        "        return self.theta[1:]  #remind that theta is (w0, w1, w2, w3, w4.....wn)\n",
        "                               #w0 is the bias or the intercept\n",
        "                               #w1....wn are the weights / coefficients / theta\n",
        "    def _bias(self):\n",
        "        return self.theta[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create `Ridge`, `Lasso` and `Elastic` class that extends the `LinearRegression`, with added penalty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LassoPenalty:\n",
        "    \n",
        "    def __init__(self, l):\n",
        "        self.l = l # lambda value\n",
        "        \n",
        "    def __call__(self, theta): #__call__ allows us to call class as method\n",
        "        return self.l * np.sum(np.abs(theta))\n",
        "        \n",
        "    def derivation(self, theta):\n",
        "        return self.l * np.sign(theta)\n",
        "    \n",
        "class RidgePenalty:\n",
        "    \n",
        "    def __init__(self, l):\n",
        "        self.l = l\n",
        "        \n",
        "    def __call__(self, theta): #__call__ allows us to call class as method\n",
        "        return self.l * np.sum(np.square(theta))\n",
        "        \n",
        "    def derivation(self, theta):\n",
        "        return self.l * 2 * theta\n",
        "    \n",
        "class ElasticPenalty:\n",
        "    \n",
        "    def __init__(self, l = 0.1, l_ratio = 0.5):\n",
        "        self.l = l \n",
        "        self.l_ratio = l_ratio\n",
        "\n",
        "    def __call__(self, theta):  #__call__ allows us to call class as method\n",
        "        l1_contribution = self.l_ratio * self.l * np.sum(np.abs(theta))\n",
        "        l2_contribution = (1 - self.l_ratio) * self.l * 0.5 * np.sum(np.square(theta))\n",
        "        return (l1_contribution + l2_contribution)\n",
        "\n",
        "    def derivation(self, theta):\n",
        "        l1_derivation = self.l * self.l_ratio * np.sign(theta)\n",
        "        l2_derivation = self.l * (1 - self.l_ratio) * theta\n",
        "        return (l1_derivation + l2_derivation)\n",
        "    \n",
        "class Lasso(LinearRegression):\n",
        "    \n",
        "    def __init__(self, method, lr, theta, momentum, l):\n",
        "        self.regularization = LassoPenalty(l)\n",
        "        super().__init__(self.regularization, lr, method, theta, momentum)\n",
        "        \n",
        "class Ridge(LinearRegression):\n",
        "    \n",
        "    def __init__(self, method, lr, theta, momentum, l):\n",
        "        self.regularization = RidgePenalty(l)\n",
        "        super().__init__(self.regularization, lr, method, theta, momentum)\n",
        "        \n",
        "class ElasticNet(LinearRegression):\n",
        "    \n",
        "    def __init__(self, method, lr, l, theta, momentum, l_ratio=0.5):\n",
        "        self.regularization = ElasticPenalty(l, l_ratio)\n",
        "        super().__init__(self.regularization, lr, method, theta, momentum)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Experiment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#helper function for looping classnames\n",
        "import sys\n",
        "\n",
        "def str_to_class(classname):\n",
        "    return getattr(sys.modules[__name__], classname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "regs = [\"Ridge\", \"Lasso\", \"ElasticNet\"]\n",
        "momentums = [False, True]\n",
        "methods = [\"batch\"]\n",
        "thetas = ['zero', 'xavier']\n",
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "\n",
        "all_combinations = list(itertools.product(regs, momentums, methods, thetas, learning_rates))\n",
        "\n",
        "parameters = []\n",
        "\n",
        "for combo in all_combinations:\n",
        "    parameters.append({\n",
        "        \"reg\": combo[0],\n",
        "        \"momentum\": combo[1],\n",
        "        \"method\": combo[2],\n",
        "        \"theta\": combo[3],\n",
        "        \"lr\": combo[4],\n",
        "        \"l\": 0.1\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for params in parameters:\n",
        "    run_name=f\"method-{params['method']}-lr-{params['lr']}-reg-{params['reg']}-theta-{params['theta']}-momentum-{params['momentum']}\"\n",
        "    print(run_name)\n",
        "    mlflow.start_run(run_name=f\"method-{params['method']}-lr-{params['lr']}-reg-{params['reg']}-theta-{params['theta']}-momentum-{params['momentum']}\", nested=True)\n",
        "\n",
        "    type_of_regression = str_to_class(params['reg'])    #Ridge, Lasso, ElasticNet\n",
        "    del params[\"reg\"]\n",
        "    model = type_of_regression(**params)  \n",
        "    model.fit(X_train, y_train.values)\n",
        "    yhat = model.predict(X_test)\n",
        "    mse  = model.mse(yhat, y_test)\n",
        "    r2 = model.r2(yhat, y_test)\n",
        "\n",
        "    mlflow.log_metric(key=\"test_mse\", value=mse)\n",
        "    mlflow.log_metric(key=\"r2\", value=r2)\n",
        "\n",
        "    signature = mlflow.models.infer_signature(X_train, model.predict(X_train))\n",
        "    mlflow.sklearn.log_model(model, artifact_path='model', signature=signature)\n",
        "\n",
        "    mlflow.end_run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(len(parameters))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "*****************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijiqs9AfgD2c"
      },
      "source": [
        "**7. Testing**\n",
        "\n",
        "As the best MSE value has been satisfactory to our need, the model is tested on test set below. The X_test has been already scaled and y_test has been logarithm scaled like the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsyx4qkdgFdx",
        "outputId": "0e3653e0-75db-49c2-d538-81ef5391009a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "yhat = grid.predict(X_test)\n",
        "\n",
        "mean_squared_error(yhat, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The mean squared error from the test set is similar to best mse from the training set. Hence, the model has been successfully trained. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**8. Feature Analysis**\n",
        "\n",
        "The main point of feature analysis is to distinguish the most important and relevant features in the model. Each feature may provide different level of significance in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8.1. Using the available grid methods\n",
        "\n",
        "The grid also provides the level of importance of each features used in training the model along with the best parameters.\n",
        "\n",
        "Each features is given a score from 0 t0 1 on their importance in training the data. The values are plotted in bar graph below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_best_estimator = grid.best_estimator_\n",
        "\n",
        "# Extracting the feature importance scores from the grid\n",
        "xgb_best_estimator.feature_importances_\n",
        "\n",
        "# Bar plot for the features and thier importance\n",
        "plt.barh(X.columns, xgb_best_estimator.feature_importances_)\n",
        "plt.xlabel(\"XGB Regressor Feature Importance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8.2. Using SHAP\n",
        "\n",
        "The SHAP libaray is a model agnostic library. It calculates shap values which considers all possible combinations of features and their contributions to the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "explainer = shap.TreeExplainer(xgb_best_estimator)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", feature_names = X.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From above analysis, feature max_power has the most siginifance in predicting the selling price. Both features mileage and engine have significantly less affect in prediction compared to max_power. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7mE9cdqhJjA"
      },
      "source": [
        "**9. Inference**\n",
        "\n",
        "As our model has been trained to fit our needs, the model will exported to model file using the pickle library. The model will be accesible to the app to be imported and predict the selling price based on user inputs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ2oEeYuhQxT"
      },
      "outputs": [],
      "source": [
        "# Importing pickle library\n",
        "import pickle\n",
        "\n",
        "# Exporting the model to selling-price.model\n",
        "filename = '../model/selling-price.model'\n",
        "pickle.dump(grid, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We will also dump the scaler values for future use\n",
        "scaler_filename = '../model/scaler.pkl'\n",
        "pickle.dump(scaler, open(scaler_filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use the model, it can be imported and be provided with the feature values as below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing the model\n",
        "selling_price_model = pickle.load(open(\"../model/selling-price.model\", \"rb\"))\n",
        "\n",
        "# Creating a dummy sample\n",
        "sample = {\n",
        "    \"max_power\": [100],\n",
        "    \"engine\": [1200],\n",
        "    \"mileage\": [23]\n",
        "}\n",
        "\n",
        "# Convert the sample to panda dataframe\n",
        "sample = pd.DataFrame(sample)\n",
        "\n",
        "# Scale the sample using the same scaler used for X_train and X_set\n",
        "scaled_sample = scaler.transform(sample)\n",
        "\n",
        "# Use the model to predict the selling price\n",
        "predicted_selling_price = selling_price_model.predict(scaled_sample)\n",
        "\n",
        "# As the we have log transformed the y while training and set, we will need to exponent transform the predicted value for correct prediction\n",
        "predicted_selling_price = np.exp(predicted_selling_price)\n",
        "\n",
        "print(\"The predicted selling price is \" + str(predicted_selling_price[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**10. Report**\n",
        "\n",
        "10.1. Summary\n",
        "\n",
        "The above implemenation has resulted in creating a model to meet the requirements of the car company. The prediction result from the model have been found to be good. The major blocks of the above implemenation are:\n",
        "\n",
        "- Number of final samples used: 8033\n",
        "- Split ratio for training and test: 7:3\n",
        "- Features chosen for training model: max_power, mileage, engine\n",
        "- Feature to be predicted: selling_price\n",
        "- Scaler Used: Robust Scaler\n",
        "- Algorithm Chosen to train the model: XGBRegressor\n",
        "- Best MSE result for the test set: -0.087 (Negative Squared Mean Error)\n",
        "\n",
        "\n",
        "10.2. Features Discussion\n",
        "\n",
        "There were 12 features to be considered for training our model. The feature torque was dropped earlier on due to insigifance and lack of knowledge to the car company. The categorical features were label encoded and the float values were extracted for the other coninous features to be plotted in the correlation matrix and PPS score graph.\n",
        "\n",
        "The correlation matrix showed strong values for year, engine and max_power. The PPS score graph showed strong values for engine, mileage and max_power. The features for training were selected the based on PPS score graph, and the general knowledge that these feature do signifacntly affect the prices of car in the real world.\n",
        "\n",
        "In the feature analysis section, the feature max_power has been shown to have significant importance while predicting the result. The feature engine and mileage have comparably equal importance in prediction. Hence, these features have been appropriate for training the model.\n",
        "\n",
        "\n",
        "12.3. Preprocessing Discussion\n",
        "\n",
        "The feature engine had significantly more outliers than the other two features. A high number of outliers may affect the training and prediction result of the model. However, the feature was deemed important in order to be dropped. The scaling process was relied upon to mitigate the effects of any outliers on the training. The following three scalers were considered:\n",
        "\n",
        "- Standard Scaler\n",
        "- Min Max Scaler\n",
        "- Robust Scaler\n",
        "\n",
        "From the above list, Min Max Scaler was found to be too sensitive to outliers. Standard Scaler could perform well but it was too sensitive to extreme outliers. Hence, Robust Scaler was used which was found to be less sensitive to extreme outliers.\n",
        "\n",
        "\n",
        "10.4 Algorithm Selection Discussion\n",
        "\n",
        "In order to select the appropriate model for training, the cross validation method was used to get the best mean squared error among different algoriths. The list of the algorithms and their mean scores from cross validation methods are as follows:\n",
        "\n",
        "| Algorthm | Mean from Cross Validaton |\n",
        "| -------- | ------------------------- |\n",
        "| Linear Regression | -0.2812275460816783 |\n",
        "| SVR | -0.22900504420396048 |\n",
        "| KNeighbors Regressor | -0.10405329584844414 |\n",
        "| Decision-Tree Regressor | -0.09731487031196318 |\n",
        "| Random-Forest Regressor | -0.08972391099371599 |\n",
        "| XGBregressor |-0.08678452365550038 |\n",
        "\n",
        "*Scores are from 29th August 2023. Scores are subjected to change if the cross validation is run again*\n",
        "\n",
        "As from the above table, the best values were acheived from Random Foreset Regressor and XGBRegressor with latter scoring better with some magnitude. Both algorithm were trained and tested using equivalent paramters and XBG was found to be produce consistent better results. Hence, XBG Regressor was chose for the final implmentation.\n",
        "\n",
        "XGB Regressor has been found to be more strong boosting and strong predictive algorithm which could handle our dataset. The parameters for the XBG Regressor were set as follows:\n",
        "\n",
        "- max_depth: [5, 10, None],\n",
        "- n_estimators: [100, 200, 300, 400, 500],\n",
        "- learning_rate: [0.1, 0.2]\n",
        "\n",
        "Grid Search was applied to find the best parameter values and best mse score from the algorithm that would train our model. Following were results:\n",
        "\n",
        "Best value for: \n",
        "- learning_rate: 0.1\n",
        "- max_depth: None\n",
        "- n_estimators: 500\n",
        "\n",
        "Best MSE from grid: -0.08626911482197705\n",
        "\n",
        "\n",
        "10.5 Testing and Conclusion\n",
        "\n",
        "As the final step in our implementaion, the model was tested using the split test. The MSE for the test was found to be **0.08371498311349586**\n",
        "\n",
        "The training and testing of model was successful due to favorable results achieved above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
