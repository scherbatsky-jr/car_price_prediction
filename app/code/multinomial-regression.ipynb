{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.24.4', '1.5.3', '0.12.2', '3.7.2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, pd.__version__, sns.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preproccesed data\n",
    "\n",
    "df = pd.read_csv('../dataset/prepocessed_cars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_power</th>\n",
       "      <th>engine</th>\n",
       "      <th>mileage</th>\n",
       "      <th>selling_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.40</td>\n",
       "      <td>2499.0</td>\n",
       "      <td>13.58</td>\n",
       "      <td>12.980800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62.10</td>\n",
       "      <td>2523.0</td>\n",
       "      <td>15.96</td>\n",
       "      <td>12.899220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.76</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>20.77</td>\n",
       "      <td>13.122363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.00</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>18.80</td>\n",
       "      <td>12.154779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.00</td>\n",
       "      <td>1396.0</td>\n",
       "      <td>23.00</td>\n",
       "      <td>12.278393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_power  engine  mileage  selling_price\n",
       "0      72.40  2499.0    13.58      12.980800\n",
       "1      62.10  2523.0    15.96      12.899220\n",
       "2      88.76  1248.0    20.77      13.122363\n",
       "3      90.00  1248.0    18.80      12.154779\n",
       "4      90.00  1396.0    23.00      12.278393"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['selling_price'] = np.exp(df['selling_price'])\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Converting selling price into discrete values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.308919326755392\n",
      "16.11809565095832\n"
     ]
    }
   ],
   "source": [
    "print(min(df['selling_price']))\n",
    "print(max(df['selling_price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_power</th>\n",
       "      <th>engine</th>\n",
       "      <th>mileage</th>\n",
       "      <th>selling_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.40</td>\n",
       "      <td>2499.0</td>\n",
       "      <td>13.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62.10</td>\n",
       "      <td>2523.0</td>\n",
       "      <td>15.96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.76</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>20.77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.00</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>18.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.00</td>\n",
       "      <td>1396.0</td>\n",
       "      <td>23.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_power  engine  mileage selling_price\n",
       "0      72.40  2499.0    13.58             1\n",
       "1      62.10  2523.0    15.96             1\n",
       "2      88.76  1248.0    20.77             1\n",
       "3      90.00  1248.0    18.80             1\n",
       "4      90.00  1396.0    23.00             1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bins = [29999, 2500000, 5000000, 7500000, 100000000]\n",
    "bins = [10, 11.75, 13.5, 15.25, 17]\n",
    "labels = [0, 1, 2, 3]\n",
    "\n",
    "df['selling_price'] = pd.cut(df['selling_price'], bins=bins, labels=labels)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the selling price column has been categorized into four categories (0, 1, 2, 3) based on range. Lets see the number of categories for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of category 0: 550\n",
      "Number of category 1: 5752\n",
      "Number of category 2: 1606\n",
      "Number of category 3: 120\n"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    print(f\"Number of category {label}: {(df['selling_price'] == label).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['max_power', 'engine', 'mileage']]\n",
    "\n",
    "y = df['selling_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add intercept to our X\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train   = np.concatenate((intercept, X_train), axis=1)  #add intercept\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test    = np.concatenate((intercept, X_test), axis=1)  #add intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_encoded = pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = len(set(y))  # no. of class  (can also use np.unique)\n",
    "m = X_train.shape[0]  # no.of samples\n",
    "n = X_train.shape[1]  # no. of features\n",
    "Y_train_encoded = np.zeros((m, k))\n",
    "for each_class in range(k):\n",
    "    cond = y_train==each_class\n",
    "    Y_train_encoded[np.where(cond), each_class] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (5619, 4)\n",
      "Shape of X_test:  (2409, 4)\n",
      "Shape of y_train:  (5619,)\n",
      "Shape of y_test:  (2409,)\n"
     ]
    }
   ],
   "source": [
    "# Shape check for X_train, X_test, y_train, y before model fitting\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5867    1\n",
       "5957    2\n",
       "3032    1\n",
       "6445    0\n",
       "318     1\n",
       "       ..\n",
       "5670    2\n",
       "1003    1\n",
       "4801    2\n",
       "7446    0\n",
       "555     1\n",
       "Name: selling_price, Length: 2409, dtype: category\n",
       "Categories (4, int64): [0 < 1 < 2 < 3]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, regularization, method, alpha, k, n, max_iter=5000):\n",
    "        self.regularization = regularization\n",
    "        self.k = k\n",
    "        self.n = n\n",
    "        self.method = method\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        self.W = np.random.rand(self.n, self.k)\n",
    "        self.losses = []\n",
    "        \n",
    "        if self.method == \"batch\":\n",
    "            start_time = time.time()\n",
    "            for i in range(self.max_iter):\n",
    "                loss, grad =  self.gradient(X, Y)\n",
    "                self.losses.append(loss)\n",
    "                self.W = self.W - self.alpha * grad\n",
    "                if i % 500 == 0:\n",
    "                    print(f\"Loss at iteration {i}\", loss)\n",
    "            print(f\"time taken: {time.time() - start_time}\")\n",
    "            \n",
    "        elif self.method == \"minibatch\":\n",
    "            start_time = time.time()\n",
    "            batch_size = int(0.3 * X.shape[0])\n",
    "            for i in range(self.max_iter):\n",
    "                ix = np.random.randint(0, X.shape[0]) #<----with replacement\n",
    "                batch_X = X[ix:ix+batch_size]\n",
    "                batch_Y = Y[ix:ix+batch_size]\n",
    "                loss, grad = self.gradient(batch_X, batch_Y)\n",
    "                self.losses.append(loss)\n",
    "                self.W = self.W - self.alpha * grad\n",
    "                if i % 500 == 0:\n",
    "                    print(f\"Loss at iteration {i}\", loss)\n",
    "            print(f\"time taken: {time.time() - start_time}\")\n",
    "            \n",
    "        elif self.method == \"stochastic\":\n",
    "            start_time = time.time()\n",
    "            list_of_used_ix = []\n",
    "            for i in range(self.max_iter):\n",
    "                idx = np.random.randint(X.shape[0])\n",
    "                while i in list_of_used_ix:\n",
    "                    idx = np.random.randint(X.shape[0])\n",
    "                X_train = X[idx, :].reshape(1, -1)\n",
    "                Y_train = Y[idx]\n",
    "                loss, grad = self.gradient(X_train, Y_train)\n",
    "                self.losses.append(loss)\n",
    "                self.W = self.W - self.alpha * grad\n",
    "                \n",
    "                list_of_used_ix.append(i)\n",
    "                if len(list_of_used_ix) == X.shape[0]:\n",
    "                    list_of_used_ix = []\n",
    "                if i % 500 == 0:\n",
    "                    print(f\"Loss at iteration {i}\", loss)\n",
    "            print(f\"time taken: {time.time() - start_time}\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Method must be one of the followings: \"batch\", \"minibatch\" or \"sto\".')\n",
    "        \n",
    "        \n",
    "    def gradient(self, X, Y):\n",
    "        m = X.shape[0]\n",
    "        h = self.h_theta(X, self.W)\n",
    "        loss = - np.sum(Y*np.log(h)) / m + self.regularization(self.W)\n",
    "        error = h - Y\n",
    "        grad = self.softmax_grad(X, error) + self.regularization.derivation(self.W)\n",
    "        return loss, grad\n",
    "\n",
    "    def softmax(self, theta_t_x):\n",
    "        return np.exp(theta_t_x) / np.sum(np.exp(theta_t_x), axis=1, keepdims=True)\n",
    "\n",
    "    def softmax_grad(self, X, error):\n",
    "        return  X.T @ error\n",
    "\n",
    "    def h_theta(self, X, W):\n",
    "        return self.softmax(X @ W)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return np.argmax(self.h_theta(X_test, self.W), axis=1)\n",
    "    \n",
    "    def plot(self):\n",
    "        plt.plot(np.arange(len(self.losses)) , self.losses, label = \"Train Losses\")\n",
    "        plt.title(\"Losses\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"losses\")\n",
    "        plt.legend()\n",
    "\n",
    "    def accuracy(self, y, y_pred):\n",
    "        correct_predictions = np.sum(y == y_pred)\n",
    "        total_predictions = y.shape[0]\n",
    "        return correct_predictions / total_predictions\n",
    "\n",
    "    def precision(self, y, y_pred, c=0):\n",
    "        true_positives = np.sum((y == c) & (y_pred == c))\n",
    "        false_positives = np.sum((y != c) & (y_pred == c))\n",
    "        if true_positives + false_positives == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return true_positives / (true_positives + false_positives)\n",
    "\n",
    "    def recall(self, y, y_pred, c=0):\n",
    "        true_positives = np.sum((y == c) & (y_pred == c))\n",
    "        false_negatives = np.sum((y == c) & (y_pred != c))\n",
    "        if true_positives + false_negatives == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    def f1_score(self, y, y_pred, c=0):\n",
    "        precision = self.precision(y, y_pred, c)\n",
    "        recall = self.recall(y, y_pred, c)\n",
    "        if precision + recall == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    def macro_precision(self, y, y_pred):\n",
    "        precisions = [self.precision(y, y_pred, c) for c in range(self.k)]\n",
    "        return np.sum(precisions) / self.k\n",
    "\n",
    "    def macro_recall(self, y, y_pred):\n",
    "        recalls = [self.recall(y, y_pred, c) for c in range(self.k)]\n",
    "        return np.sum(recalls) / self.k\n",
    "\n",
    "    def macro_f1(self, y, y_pred):\n",
    "        f1s = [self.f1_score(y, y_pred, c) for c in range(self.k)]\n",
    "        return np.sum(f1s) / self.k\n",
    "\n",
    "    def weighted_precision(self, y, y_pred):\n",
    "        class_counts = [np.count_nonzero(y == c) for c in range(self.k)]\n",
    "        precisions = [class_counts[c] / len(y) * self.precision(y, y_pred, c) for c in range(self.k)]\n",
    "        return np.sum(precisions)\n",
    "\n",
    "    def weighted_recall(self, y, y_pred):\n",
    "        class_counts = [np.count_nonzero(y == c) for c in range(self.k)]\n",
    "        recalls = [class_counts[c] / len(y) * self.recall(y, y_pred, c) for c in range(self.k)]\n",
    "        return np.sum(recalls)\n",
    "\n",
    "    def weighted_f1(self, y, y_pred):\n",
    "        class_counts = [np.count_nonzero(y == c) for c in range(self.k)]\n",
    "        f1s = [class_counts[c] / len(y) * self.f1_score(y, y_pred, c) for c in range(self.k)]\n",
    "        return np.sum(f1s)\n",
    "\n",
    "    def classification_report(self, y, y_pred):\n",
    "        cols = [\"precision\", \"recall\", \"f1-score\"]\n",
    "        idx = list(range(self.k)) + [\"accuracy\", \"macro\", \"weighted\"]\n",
    "\n",
    "        report = [[self.precision(y, y_pred, c),\n",
    "                   self.recall(y, y_pred, c),\n",
    "                   self.f1_score(y, y_pred, c)] for c in range(self.k)]\n",
    "\n",
    "        report.append([\"\", \"\", self.accuracy(y, y_pred)])\n",
    "\n",
    "        report.append([self.macro_precision(y, y_pred),\n",
    "                       self.macro_recall(y, y_pred),\n",
    "                       self.macro_f1(y, y_pred)])\n",
    "\n",
    "        report.append([self.weighted_precision(y, y_pred),\n",
    "                       self.weighted_recall(y, y_pred),\n",
    "                       self.weighted_f1(y, y_pred)])\n",
    "\n",
    "        return pd.DataFrame(report, index=idx, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgePenalty:\n",
    "    def __init__(self, l):\n",
    "        self.l = l\n",
    "        \n",
    "    def __call__(self, theta): \n",
    "        return self.l * np.sum(np.square(theta))\n",
    "        \n",
    "    def derivation(self, theta):\n",
    "        return self.l * 2 * theta\n",
    "    \n",
    "class NoPenalty():\n",
    "    def __init__(self, l):\n",
    "        self.l = l\n",
    "        \n",
    "    def __call__(self, theta): \n",
    "        return 0.0\n",
    "        \n",
    "    def derivation(self, theta):\n",
    "        return 0.0\n",
    "    \n",
    "class Ridge(LogisticRegression):\n",
    "    def __init__(self, reg, method, alpha, k, n, l):\n",
    "        self.regularization = RidgePenalty(l)\n",
    "        super().__init__(self.regularization, method, alpha, k, n)\n",
    "\n",
    "class Normal(LogisticRegression):\n",
    "    def __init__(self, reg, method, alpha, k, n, l):\n",
    "        self.regularization = NoPenalty(l)\n",
    "        super().__init__(self.regularization, method, alpha, k, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "regs = ['Normal', 'Ridge']\n",
    "methods = ['stochastic', 'minibatch', 'batch']\n",
    "alphas = [0.01, 0.001, 0.0001]\n",
    "\n",
    "# Combining all the parameters into list of arrays\n",
    "all_combinations = list(itertools.product(regs, methods, alphas))\n",
    "\n",
    "# Init parameters\n",
    "parameters = []\n",
    "\n",
    "# Fill the parameters list with combination dictionaries\n",
    "for combo in all_combinations:\n",
    "    parameters.append({\n",
    "        \"reg\": combo[0],\n",
    "        \"method\": combo[1],\n",
    "        \"alpha\": combo[2],\n",
    "        \"k\": k,\n",
    "        \"n\": n,\n",
    "        \"l\": 0.1\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for looping classnames\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg-Normal-method-stochastic-alpha=0.01\n",
      "Loss at iteration 0 2.502923880772256\n",
      "Loss at iteration 500 0.5717060146710765\n",
      "Loss at iteration 1000 0.2654845448740318\n",
      "Loss at iteration 1500 1.95270627686727\n",
      "Loss at iteration 2000 1.4362431504272961\n",
      "Loss at iteration 2500 0.24536842860085137\n",
      "Loss at iteration 3000 0.3213732181768244\n",
      "Loss at iteration 3500 0.18346973720281684\n",
      "Loss at iteration 4000 0.19326321075148017\n",
      "Loss at iteration 4500 1.0859428677440905\n",
      "time taken: 0.1685619354248047\n",
      "[1 2 1 ... 2 1 1]\n",
      "========================================\n",
      "reg-Normal-method-stochastic-alpha=0.001\n",
      "Loss at iteration 0 1.0740483417771054\n",
      "Loss at iteration 500 1.7803721946079025\n",
      "Loss at iteration 1000 0.733874381103748\n",
      "Loss at iteration 1500 0.8464456280782063\n",
      "Loss at iteration 2000 0.4876786545719561\n",
      "Loss at iteration 2500 1.7985672611634838\n",
      "Loss at iteration 3000 0.6697118262171395\n",
      "Loss at iteration 3500 0.401820617873111\n",
      "Loss at iteration 4000 1.9199208692767806\n",
      "Loss at iteration 4500 0.3287383360730625\n",
      "time taken: 0.15339183807373047\n",
      "[1 2 1 ... 2 1 1]\n",
      "========================================\n",
      "reg-Normal-method-stochastic-alpha=0.0001\n",
      "Loss at iteration 0 1.260136707635541\n",
      "Loss at iteration 500 1.6394634978071616\n",
      "Loss at iteration 1000 1.6824086424032394\n",
      "Loss at iteration 1500 1.5882983594267874\n",
      "Loss at iteration 2000 1.6485724620155648\n",
      "Loss at iteration 2500 1.4124262348965355\n",
      "Loss at iteration 3000 1.5975959931246948\n",
      "Loss at iteration 3500 1.6590725082014983\n",
      "Loss at iteration 4000 1.5325808539875174\n",
      "Loss at iteration 4500 1.4108111649023247\n",
      "time taken: 0.1540231704711914\n",
      "[0 2 1 ... 2 0 0]\n",
      "========================================\n",
      "reg-Normal-method-minibatch-alpha=0.01\n",
      "Loss at iteration 0 1.654554126410557\n",
      "Loss at iteration 500 3.18581540728019\n",
      "Loss at iteration 1000 2.9448084229230913\n",
      "Loss at iteration 1500 3.1586463460089664\n",
      "Loss at iteration 2000 2.8906731003064188\n",
      "Loss at iteration 2500 1.2418377899567186\n",
      "Loss at iteration 3000 0.9037827709114827\n",
      "Loss at iteration 3500 1.227360850799029\n",
      "Loss at iteration 4000 3.1072277065544758\n",
      "Loss at iteration 4500 3.7355881502061723\n",
      "time taken: 0.5656740665435791\n",
      "[1 2 2 ... 2 1 1]\n",
      "========================================\n",
      "reg-Normal-method-minibatch-alpha=0.001\n",
      "Loss at iteration 0 1.441874144899955\n",
      "Loss at iteration 500 0.5188418859913894\n",
      "Loss at iteration 1000 0.5045999511636096\n",
      "Loss at iteration 1500 0.5177934024329996\n",
      "Loss at iteration 2000 0.5284862177771533\n",
      "Loss at iteration 2500 0.5096117268183137\n",
      "Loss at iteration 3000 0.5402808890923706\n",
      "Loss at iteration 3500 0.4997431804629682\n",
      "Loss at iteration 4000 0.5074470912838817\n",
      "Loss at iteration 4500 0.5229653637053924\n",
      "time taken: 0.5478458404541016\n",
      "[1 2 1 ... 2 1 1]\n",
      "========================================\n",
      "reg-Normal-method-minibatch-alpha=0.0001\n",
      "Loss at iteration 0 1.6494852021541249\n",
      "Loss at iteration 500 0.5565735055314923\n",
      "Loss at iteration 1000 0.5235176580106935\n",
      "Loss at iteration 1500 0.5144398570662407\n",
      "Loss at iteration 2000 0.5160159307958602\n",
      "Loss at iteration 2500 0.5141476319602438\n",
      "Loss at iteration 3000 0.524621740770882\n",
      "Loss at iteration 3500 0.5316293290215228\n",
      "Loss at iteration 4000 0.5229900453038548\n",
      "Loss at iteration 4500 0.4979953746366593\n",
      "time taken: 0.586622953414917\n",
      "[1 2 1 ... 2 1 1]\n",
      "========================================\n",
      "reg-Normal-method-batch-alpha=0.01\n",
      "Loss at iteration 0 1.3738034845772604\n",
      "Loss at iteration 500 10.97619574582598\n",
      "Loss at iteration 1000 10.814810124134333\n",
      "Loss at iteration 1500 nan\n",
      "Loss at iteration 2000 nan\n",
      "Loss at iteration 2500 nan\n",
      "Loss at iteration 3000 nan\n",
      "Loss at iteration 3500 nan\n",
      "Loss at iteration 4000 nan\n",
      "Loss at iteration 4500 nan\n",
      "time taken: 1.8963651657104492\n",
      "[1 2 2 ... 2 1 1]\n",
      "========================================\n",
      "reg-Normal-method-batch-alpha=0.001\n",
      "Loss at iteration 0 1.5941881288335153\n",
      "Loss at iteration 500 0.8490627919688767\n",
      "Loss at iteration 1000 0.85319536220421\n",
      "Loss at iteration 1500 0.8569744460496087\n",
      "Loss at iteration 2000 0.8593700538740937\n",
      "Loss at iteration 2500 0.8605736905688017\n",
      "Loss at iteration 3000 0.8611121106014937\n",
      "Loss at iteration 3500 0.8613657218330809\n",
      "Loss at iteration 4000 0.8615042222991518\n",
      "Loss at iteration 4500 0.8615902759663376\n",
      "time taken: 1.9423038959503174\n",
      "[1 1 1 ... 1 1 1]\n",
      "========================================\n",
      "reg-Normal-method-batch-alpha=0.0001\n",
      "Loss at iteration 0 1.2673820762162524\n",
      "Loss at iteration 500 0.5231166091998606\n",
      "Loss at iteration 1000 0.5184143420036466\n",
      "Loss at iteration 1500 0.5169901286407385\n",
      "Loss at iteration 2000 0.5163312458703476\n",
      "Loss at iteration 2500 0.5159633563034465\n",
      "Loss at iteration 3000 0.5157350541370017\n",
      "Loss at iteration 3500 0.5155835257447292\n",
      "Loss at iteration 4000 0.5154781713962256\n",
      "Loss at iteration 4500 0.5154023852938885\n",
      "time taken: 1.8856399059295654\n",
      "[1 2 1 ... 2 1 1]\n",
      "========================================\n",
      "reg-Ridge-method-stochastic-alpha=0.01\n",
      "Loss at iteration 0 1.4805621206398674\n",
      "Loss at iteration 500 0.7263376704023188\n",
      "Loss at iteration 1000 0.8463907648864192\n",
      "Loss at iteration 1500 1.5655108903859924\n",
      "Loss at iteration 2000 1.4267479379324484\n",
      "Loss at iteration 2500 0.6878427249377705\n",
      "Loss at iteration 3000 1.749472276780784\n",
      "Loss at iteration 3500 0.7360364460109139\n",
      "Loss at iteration 4000 0.6885106762812392\n",
      "Loss at iteration 4500 0.6276517879048218\n",
      "time taken: 0.16671204566955566\n",
      "[1 2 1 ... 2 1 1]\n",
      "========================================\n",
      "reg-Ridge-method-stochastic-alpha=0.001\n",
      "Loss at iteration 0 2.065379509052676\n",
      "Loss at iteration 500 1.668327745625351\n",
      "Loss at iteration 1000 1.2811202187758048\n",
      "Loss at iteration 1500 1.3008109614071015\n",
      "Loss at iteration 2000 1.2160811933612874\n",
      "Loss at iteration 2500 0.9740794569412674\n",
      "Loss at iteration 3000 2.109778949307759\n",
      "Loss at iteration 3500 0.9974373562256176\n",
      "Loss at iteration 4000 0.8358573168972907\n",
      "Loss at iteration 4500 0.8456835334204718\n",
      "time taken: 0.16913795471191406\n",
      "[1 2 1 ... 2 1 1]\n",
      "========================================\n",
      "reg-Ridge-method-stochastic-alpha=0.0001\n",
      "Loss at iteration 0 3.312147321951615\n",
      "Loss at iteration 500 2.031454606235914\n",
      "Loss at iteration 1000 2.3159347581334564\n",
      "Loss at iteration 1500 2.6965009040007235\n",
      "Loss at iteration 2000 2.4810553890335187\n",
      "Loss at iteration 2500 1.8120496893953884\n",
      "Loss at iteration 3000 1.9846270054652575\n",
      "Loss at iteration 3500 1.9026529313190497\n",
      "Loss at iteration 4000 1.8649933737925972\n",
      "Loss at iteration 4500 1.6092010363010951\n",
      "time taken: 0.1772150993347168\n",
      "[1 1 1 ... 1 3 1]\n",
      "========================================\n",
      "reg-Ridge-method-minibatch-alpha=0.01\n",
      "Loss at iteration 0 2.053985678549798\n",
      "Loss at iteration 500 43.562318142581205\n",
      "Loss at iteration 1000 51.033274300500494\n",
      "Loss at iteration 1500 47.4728097039636\n",
      "Loss at iteration 2000 50.12188934806536\n",
      "Loss at iteration 2500 46.812563132053256\n",
      "Loss at iteration 3000 38.963398027838494\n",
      "Loss at iteration 3500 46.102207390725226\n",
      "Loss at iteration 4000 49.37223626511554\n",
      "Loss at iteration 4500 44.32822901272984\n",
      "time taken: 0.6588280200958252\n",
      "[1 2 2 ... 2 1 1]\n",
      "========================================\n",
      "reg-Ridge-method-minibatch-alpha=0.001\n",
      "Loss at iteration 0 2.1901715018999632\n",
      "Loss at iteration 500 6.072040891043585\n",
      "Loss at iteration 1000 6.911204073696927\n",
      "Loss at iteration 1500 7.22018739069251\n",
      "Loss at iteration 2000 7.339556340889985\n",
      "Loss at iteration 2500 7.365489885988782\n",
      "Loss at iteration 3000 7.35838267679641\n",
      "Loss at iteration 3500 7.366424452996196\n",
      "Loss at iteration 4000 7.315810443120711\n",
      "Loss at iteration 4500 7.307780905274053\n",
      "time taken: 0.5880739688873291\n",
      "[1 2 1 ... 2 1 1]\n",
      "========================================\n",
      "reg-Ridge-method-minibatch-alpha=0.0001\n",
      "Loss at iteration 0 1.9608850588108346\n",
      "Loss at iteration 500 2.774948172768522\n",
      "Loss at iteration 1000 3.5946858750779445\n",
      "Loss at iteration 1500 4.182686560848378\n",
      "Loss at iteration 2000 4.6333346124617965\n",
      "Loss at iteration 2500 4.962011594249827\n",
      "Loss at iteration 3000 5.259655174635146\n",
      "Loss at iteration 3500 5.500271492025787\n",
      "Loss at iteration 4000 5.702937296793643\n",
      "Loss at iteration 4500 5.8898655018189165\n",
      "time taken: 0.6127710342407227\n",
      "[1 2 1 ... 2 1 1]\n",
      "========================================\n",
      "reg-Ridge-method-batch-alpha=0.01\n",
      "Loss at iteration 0 1.8702806172676185\n",
      "Loss at iteration 500 522.9953312230499\n",
      "Loss at iteration 1000 520.7020961582091\n",
      "Loss at iteration 1500 519.1078924686688\n",
      "Loss at iteration 2000 525.3355711208656\n",
      "Loss at iteration 2500 520.1785022629322\n",
      "Loss at iteration 3000 519.3208612056254\n",
      "Loss at iteration 3500 520.9318770724767\n",
      "Loss at iteration 4000 527.2955791513183\n",
      "Loss at iteration 4500 521.4222719207162\n",
      "time taken: 2.0056068897247314\n",
      "[1 1 1 ... 1 0 1]\n",
      "========================================\n",
      "reg-Ridge-method-batch-alpha=0.001\n",
      "Loss at iteration 0 2.0562505617264355\n",
      "Loss at iteration 500 14.938178095281948\n",
      "Loss at iteration 1000 19.05832554583905\n",
      "Loss at iteration 1500 21.76411661398948\n",
      "Loss at iteration 2000 23.64305614216573\n",
      "Loss at iteration 2500 24.966428415784957\n",
      "Loss at iteration 3000 25.900373094792197\n",
      "Loss at iteration 3500 26.558236308173232\n",
      "Loss at iteration 4000 27.020244704392717\n",
      "Loss at iteration 4500 27.343678761940243\n",
      "time taken: 1.9202640056610107\n",
      "[1 1 1 ... 1 1 1]\n",
      "========================================\n",
      "reg-Ridge-method-batch-alpha=0.0001\n",
      "Loss at iteration 0 1.8432117474776775\n",
      "Loss at iteration 500 4.606251825077496\n",
      "Loss at iteration 1000 5.818637358748073\n",
      "Loss at iteration 1500 6.552257431839093\n",
      "Loss at iteration 2000 7.0691601965147335\n",
      "Loss at iteration 2500 7.459256014911276\n",
      "Loss at iteration 3000 7.765355506168017\n",
      "Loss at iteration 3500 8.01162581847247\n",
      "Loss at iteration 4000 8.213185396734652\n",
      "Loss at iteration 4500 8.38018408855108\n",
      "time taken: 1.941659927368164\n",
      "[1 2 1 ... 2 1 1]\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for params in parameters:\n",
    "    run_name=f\"reg-{params['reg']}-method-{params['method']}-alpha={params['alpha']}\"\n",
    "\n",
    "    print(run_name)\n",
    "    \n",
    "    # Start mlflow experiment\n",
    "    mlflow.start_run(run_name = run_name, nested=True)\n",
    "\n",
    "    type_of_regression = str_to_class(params['reg'])\n",
    "    \n",
    "    # Instantize an regression object based on regularization. \n",
    "    model = type_of_regression(**params)\n",
    "\n",
    "    # Start training the model with training set\n",
    "    model.fit(X_train, Y_train_encoded)\n",
    "\n",
    "    yhat = model.predict(X_test)\n",
    "\n",
    "    accuracy = model.accuracy(y_test, yhat)\n",
    "    precision = model.precision(y_test, yhat)\n",
    "    recall = model.recall(y_test, yhat)\n",
    "    f1_score = model.f1_score(y_test, yhat)\n",
    "\n",
    "    print(\"Accuracy: \" + accuracy)\n",
    "    print(\"Precision: \", + precision)\n",
    "    print(\"Recall: \" + recall)\n",
    "    print(\"F1_Score: \" + f1_score)\n",
    "\n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score\n",
    "    })\n",
    "\n",
    "    # Log the trained model in mlflow\n",
    "    signature = mlflow.models.infer_signature(X_train, model.predict(X_train))\n",
    "    mlflow.sklearn.log_model(model, artifact_path='model', signature=signature)\n",
    "\n",
    "    mlflow.end_run()\n",
    "    print(\"========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
